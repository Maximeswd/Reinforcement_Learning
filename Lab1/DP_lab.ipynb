{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - Dynamic Programming\n",
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the dp_autograde.py file into codegrade.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell imports %%execwritefile command (executes cell and writes it into file). \n",
    "# All cells that start with %%execwritefile should be in dp_autograde.py file after running all cells.\n",
    "from custommagics import CustomMagics\n",
    "get_ipython().register_magics(CustomMagics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting dp_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile dp_autograde.py\n",
    "import numpy as np\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "assert sys.version_info[:3] >= (3, 6, 0), \"Make sure you have Python 3.6 installed!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Policy Evaluation (1 point)\n",
    "In this exercise we will evaluate a policy, e.g. find the value function of a policy. The problem we consider is the gridworld from Example 4.1 in the book. The environment is implemented as `GridworldEnv`, which is a subclass of the `Env` class from [OpenAI Gym](https://github.com/openai/gym). This means that we can interact with the environment. We can look at the documentation to see how we can interact with the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        GridworldEnv\n",
      "\u001b[0;31mString form:\u001b[0m <GridworldEnv instance>\n",
      "\u001b[0;31mFile:\u001b[0m        ~/Documents/Master Artificial Intelligence/UvA/Reinforcement Learning/Reinforcement Learning/Reinforcement_Learning/Lab1/gridworld.py\n",
      "\u001b[0;31mDocstring:\u001b[0m  \n",
      "Grid World environment from Sutton's Reinforcement Learning book chapter 4.\n",
      "You are an agent on an MxN grid and your goal is to reach the terminal\n",
      "state at the top left or the bottom right corner.\n",
      "\n",
      "For example, a 4x4 grid looks as follows:\n",
      "\n",
      "T  o  o  o\n",
      "o  x  o  o\n",
      "o  o  o  o\n",
      "o  o  o  T\n",
      "\n",
      "x is your position and T are the two terminal states.\n",
      "\n",
      "You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\n",
      "Actions going off the edge leave you in your current state.\n",
      "You receive a reward of -1 at each step until you reach a terminal state."
     ]
    }
   ],
   "source": [
    "from gridworld import GridworldEnv\n",
    "env = GridworldEnv()\n",
    "# Lets see what this is\n",
    "?env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mType:\u001b[0m        GridworldEnv\n",
      "\u001b[0;31mString form:\u001b[0m <GridworldEnv instance>\n",
      "\u001b[0;31mFile:\u001b[0m        ~/Documents/Master Artificial Intelligence/UvA/Reinforcement Learning/Reinforcement Learning/Reinforcement_Learning/Lab1/gridworld.py\n",
      "\u001b[0;31mSource:\u001b[0m     \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mGridworldEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdiscrete\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiscreteEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    Grid World environment from Sutton's Reinforcement Learning book chapter 4.\u001b[0m\n",
      "\u001b[0;34m    You are an agent on an MxN grid and your goal is to reach the terminal\u001b[0m\n",
      "\u001b[0;34m    state at the top left or the bottom right corner.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    For example, a 4x4 grid looks as follows:\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    T  o  o  o\u001b[0m\n",
      "\u001b[0;34m    o  x  o  o\u001b[0m\n",
      "\u001b[0;34m    o  o  o  o\u001b[0m\n",
      "\u001b[0;34m    o  o  o  T\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    x is your position and T are the two terminal states.\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m    You can take actions in each direction (UP=0, RIGHT=1, DOWN=2, LEFT=3).\u001b[0m\n",
      "\u001b[0;34m    Actions going off the edge leave you in your current state.\u001b[0m\n",
      "\u001b[0;34m    You receive a reward of -1 at each step until you reach a terminal state.\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'render.modes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ansi'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shape argument must be a list/tuple of length 2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mnA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mMAX_Y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mMAX_X\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'multi_index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterindex\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_index\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mis_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnS\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# We're stuck in a terminal state\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUP\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRIGHT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDOWN\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLEFT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;31m# Not a terminal state\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mns_up\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mMAX_X\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mns_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMAX_X\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mns_down\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mMAX_Y\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMAX_X\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mns_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mUP\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns_up\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns_up\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mRIGHT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns_right\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mDOWN\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns_down\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns_down\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0mP\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLEFT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns_left\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_done\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miternext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# Initial state distribution is uniform\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0misd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnS\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# We expose the model of the environment for educational purposes\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;31m# This should not be used in any model-free learning algorithm\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGridworldEnv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0misd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m_render\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mif\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0moutfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ansi'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mgrid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'multi_index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinished\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterindex\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmulti_index\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" x \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnS\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" T \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\" o \"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m                \u001b[0moutfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m            \u001b[0mit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miternext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m"
     ]
    }
   ],
   "source": [
    "# To have a quick look into the code\n",
    "??env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to evaluate a policy by using Dynamic Programming. For more information, see the [Intro to RL](https://drive.google.com/open?id=1opPSz5AZ_kVa1uWOdOiveNiBFiEOHjkG) book, section 4.1. This algorithm requires knowledge of the problem dynamics in the form of the transition probabilities $p(s',r|s,a)$. In general these are not available, but for our gridworld we know the dynamics and these can be accessed as `env.P`. Note that we do not need to use a discount_factor for episodic tasks but make sure your implementation can handle this correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: {0: [(1.0, 0, 0.0, True)],\n",
       "  1: [(1.0, 0, 0.0, True)],\n",
       "  2: [(1.0, 0, 0.0, True)],\n",
       "  3: [(1.0, 0, 0.0, True)]},\n",
       " 1: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 2, -1.0, False)],\n",
       "  2: [(1.0, 5, -1.0, False)],\n",
       "  3: [(1.0, 0, -1.0, True)]},\n",
       " 2: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 3, -1.0, False)],\n",
       "  2: [(1.0, 6, -1.0, False)],\n",
       "  3: [(1.0, 1, -1.0, False)]},\n",
       " 3: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 3, -1.0, False)],\n",
       "  2: [(1.0, 7, -1.0, False)],\n",
       "  3: [(1.0, 2, -1.0, False)]},\n",
       " 4: {0: [(1.0, 0, -1.0, True)],\n",
       "  1: [(1.0, 5, -1.0, False)],\n",
       "  2: [(1.0, 8, -1.0, False)],\n",
       "  3: [(1.0, 4, -1.0, False)]},\n",
       " 5: {0: [(1.0, 1, -1.0, False)],\n",
       "  1: [(1.0, 6, -1.0, False)],\n",
       "  2: [(1.0, 9, -1.0, False)],\n",
       "  3: [(1.0, 4, -1.0, False)]},\n",
       " 6: {0: [(1.0, 2, -1.0, False)],\n",
       "  1: [(1.0, 7, -1.0, False)],\n",
       "  2: [(1.0, 10, -1.0, False)],\n",
       "  3: [(1.0, 5, -1.0, False)]},\n",
       " 7: {0: [(1.0, 3, -1.0, False)],\n",
       "  1: [(1.0, 7, -1.0, False)],\n",
       "  2: [(1.0, 11, -1.0, False)],\n",
       "  3: [(1.0, 6, -1.0, False)]},\n",
       " 8: {0: [(1.0, 4, -1.0, False)],\n",
       "  1: [(1.0, 9, -1.0, False)],\n",
       "  2: [(1.0, 12, -1.0, False)],\n",
       "  3: [(1.0, 8, -1.0, False)]},\n",
       " 9: {0: [(1.0, 5, -1.0, False)],\n",
       "  1: [(1.0, 10, -1.0, False)],\n",
       "  2: [(1.0, 13, -1.0, False)],\n",
       "  3: [(1.0, 8, -1.0, False)]},\n",
       " 10: {0: [(1.0, 6, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 14, -1.0, False)],\n",
       "  3: [(1.0, 9, -1.0, False)]},\n",
       " 11: {0: [(1.0, 7, -1.0, False)],\n",
       "  1: [(1.0, 11, -1.0, False)],\n",
       "  2: [(1.0, 15, -1.0, True)],\n",
       "  3: [(1.0, 10, -1.0, False)]},\n",
       " 12: {0: [(1.0, 8, -1.0, False)],\n",
       "  1: [(1.0, 13, -1.0, False)],\n",
       "  2: [(1.0, 12, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 13: {0: [(1.0, 9, -1.0, False)],\n",
       "  1: [(1.0, 14, -1.0, False)],\n",
       "  2: [(1.0, 13, -1.0, False)],\n",
       "  3: [(1.0, 12, -1.0, False)]},\n",
       " 14: {0: [(1.0, 10, -1.0, False)],\n",
       "  1: [(1.0, 15, -1.0, True)],\n",
       "  2: [(1.0, 14, -1.0, False)],\n",
       "  3: [(1.0, 13, -1.0, False)]},\n",
       " 15: {0: [(1.0, 15, 0.0, True)],\n",
       "  1: [(1.0, 15, 0.0, True)],\n",
       "  2: [(1.0, 15, 0.0, True)],\n",
       "  3: [(1.0, 15, 0.0, True)]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Take a moment to figure out what P represents. \n",
    "# Note that this is a deterministic environment. \n",
    "# What would a stochastic environment look like?\n",
    "env.P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dp_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dp_autograde.py\n",
    "\n",
    "def policy_eval_v(policy, env, discount_factor=1.0, theta=0.00001):\n",
    "    \"\"\"\n",
    "    Evaluate a policy given an environment and a full description of the environment's dynamics.\n",
    "    \n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all states.\n",
    "        discount_factor: Gamma discount factor.\n",
    "    \n",
    "    Returns:\n",
    "        Vector of length env.nS representing the value function.\n",
    "    \"\"\"\n",
    "    # Start with an all 0 value function\n",
    "    V = np.zeros(env.nS)\n",
    "\n",
    "    # Repeat until convergence\n",
    "    while True:\n",
    "        delta = 0  # Initialize the change in value function\n",
    "\n",
    "        # Loop over all states\n",
    "        for s in range(env.nS):\n",
    "            v_s = 0  # Initialize the new value of state s\n",
    "\n",
    "            # Calculate the expected value of state s under the current policy\n",
    "            for a, phi_prob in enumerate(policy[s]):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    v_s += phi_prob * prob * (reward + discount_factor * V[next_state])\n",
    "\n",
    "            # Calculate the change in the value function for this state\n",
    "            delta = max(delta, abs(V[s] - v_s))\n",
    "\n",
    "            # Update the value function for state s\n",
    "            V[s] = v_s\n",
    "\n",
    "        # If the change in value function is smaller than theta, stop\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        , -13.99993529, -19.99990698, -21.99989761,\n",
       "       -13.99993529, -17.9999206 , -19.99991379, -19.99991477,\n",
       "       -19.99990698, -19.99991379, -17.99992725, -13.99994569,\n",
       "       -21.99989761, -19.99991477, -13.99994569,   0.        ])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's run your code, does it make sense?\n",
    "random_policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "V = policy_eval_v(random_policy, env)\n",
    "assert V.shape == (env.nS)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGiCAYAAABH4aTnAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7gElEQVR4nO3de3QUZZ7/8U+CJIGBDhdz4RJu4hJALiEIdGZ/EgQJyrpyhmUdvAQcDOqGEQirQ2ZZrssEBhQdZbmsSFTkBFHBGXSECAJHEwQCcQAxZ2HERCYdmEHSEjQJSf3+EHqnIQnpJ53uJrxf59Q5VOWpqm/RJ/SH71PVHWRZliUAAAAfCfZ3AQAA4OZC+AAAAD5F+AAAAD5F+AAAAD5F+AAAAD5F+AAAAD5F+AAAAD5F+AAAAD5F+AAAAD5F+AAAAD5lFD5Wrlypbt26KSwsTEOHDtX+/fvrHL9582bFxsYqLCxM/fr10wcffGBULAAAN7um8B7scfjYtGmT0tLSNG/ePB06dEgDBgxQUlKSzpw5U+P4nJwcTZw4UVOmTNHhw4c1btw4jRs3TkePHm1w8QAA3EyayntwkKdfLDd06FDdeeedevnllyVJ1dXViomJ0S9/+UvNnj37mvEPPvigysrKtG3bNte2YcOGaeDAgVq9enUDywcA4ObRVN6Db/FkcEVFhfLy8pSenu7aFhwcrFGjRik3N7fGfXJzc5WWlua2LSkpSVu3bq31POXl5SovL3etV1dX69y5c2rfvr2CgoI8KRkAcJOxLEvfffedOnbsqODgxrm18YcfflBFRYVXjmVZ1jXvbaGhoQoNDXXb5qv3YF/wKHz89a9/VVVVlaKioty2R0VF6csvv6xxH4fDUeN4h8NR63kyMjK0YMECT0oDAMBNUVGROnfu7PXj/vDDD+revXud72OeaNWqlS5cuOC2bd68eZo/f77bNl+9B/uCR+HDV9LT092SWmlpqbp06aKioiLZbDY/VgZJmjp1qr9LwGW7du3ydwm47OzZs/4uAVdp3bp1oxy3oqJCDodDhYWFDX5PcjqdNb6/Xd31aGo8Ch+33nqrmjVrppKSErftJSUlio6OrnGf6Ohoj8ZLNbebJMlmsxE+AkBISIi/S8BljdVSBpqCxp6m9+Z7Un2O5av3YF/w6F+ukJAQxcfHa+fOna5t1dXV2rlzp+x2e4372O12t/GSlJ2dXet4AABuBJZleWWpr6b0HuzxtEtaWpomTZqkwYMHa8iQIXrhhRdUVlamxx57TJKUnJysTp06KSMjQ5I0ffp0DR8+XM8995zGjh2rrKwsHTx4UGvXrvXulQAA4EOehofajuGJpvIe7HH4ePDBB3X27FnNnTtXDodDAwcO1Icffui6oaWwsNCtFZyQkKCNGzdqzpw5+vWvf63bb79dW7du1R133OG9qwAAwMf8ET6aynuwx5/z4Q9Op1Ph4eEqLS3lno8AkJyc7O8ScNmOHTv8XQIuu3peHf7XWO8ZV96T/va3v3nlhtP27dvfdO9vAfm0CwAAgc4fnY+mgvABAIABwoc5ntMDAAA+RecDAAADdD7MET4AADBA+DDHtAsAAPApOh8AABig82GO8AEAgAHChzmmXQAAgE/R+QAAwACdD3OEDwAADBA+zBE+AAAwQPgwxz0fAADAp+h8AABggM6HOcIHAAAGCB/mmHYBAAA+RecDAAADdD7MET4AADBA+DDHtAsAAPApOh8AABig82GO8AEAgKGbNTw0FNMuAADAp+h8AABggGkXc4QPAAAMED7MET4AADBA+DDHPR8AAMCn6HwAAGCAzoc5wgcAAAYIH+aYdgEAAD5F5wMAAAN0PswRPgAAMED4MMe0CwAATcypU6c0ZcoUde/eXS1atNBtt92mefPmqaKios79EhMTFRQU5LY8+eSTXq+PzgcAAAYCufPx5Zdfqrq6WmvWrFHPnj119OhRpaSkqKysTMuXL69z35SUFC1cuNC13rJlS6/XR/gAAMBAIIePMWPGaMyYMa71Hj16qKCgQKtWrbpu+GjZsqWio6Mbpa4rmHYBAMDPnE6n21JeXu71c5SWlqpdu3bXHffmm2/q1ltv1R133KH09HRdvHjR67UYhY+VK1eqW7duCgsL09ChQ7V///5ax2ZmZl4zfxQWFmZcMAAAgeBK56OhiyTFxMQoPDzctWRkZHi11hMnTuill17SE088Uee4hx56SBs2bNDHH3+s9PR0vfHGG3rkkUe8WotkMO2yadMmpaWlafXq1Ro6dKheeOEFJSUlqaCgQJGRkTXuY7PZVFBQ4FoPCgoyrxgAgADgzWmXoqIi2Ww21/bQ0NAax8+ePVtLly6t85jHjx9XbGysa/306dMaM2aMJkyYoJSUlDr3nTp1quvP/fr1U4cOHTRy5EidPHlSt91223Wvp748Dh/PP/+8UlJS9Nhjj0mSVq9erffff1+vvvqqZs+eXeM+QUFBjT5/BACAL3kzfNhsNrfwUZtZs2Zp8uTJdY7p0aOH689/+ctfNGLECCUkJGjt2rUe1zd06FBJP3ZO/BY+KioqlJeXp/T0dNe24OBgjRo1Srm5ubXud+HCBXXt2lXV1dUaNGiQfvOb36hv3761ji8vL3eb73I6nZ6UCQBAkxQREaGIiIh6jT19+rRGjBih+Ph4rV+/XsHBnt9pkZ+fL0nq0KGDx/vWxaNK/vrXv6qqqkpRUVFu26OiouRwOGrcp1evXnr11Vf13nvvacOGDaqurlZCQoK++eabWs+TkZHhNvcVExPjSZkAADQ6b97z4W2nT59WYmKiunTpouXLl+vs2bNyOBxu79WnT59WbGys677NkydPatGiRcrLy9OpU6f0+9//XsnJybrrrrvUv39/r9bX6I/a2u122e1213pCQoJ69+6tNWvWaNGiRTXuk56errS0NNe60+kkgAAAAkogP2qbnZ2tEydO6MSJE+rcuXON56ysrFRBQYHraZaQkBB99NFHeuGFF1RWVqaYmBiNHz9ec+bM8Xp9HoWPW2+9Vc2aNVNJSYnb9pKSknrf09G8eXPFxcXpxIkTtY4JDQ2t9WYbAABQt8mTJ1/33pBu3bq5hZ+YmBjt2bOnkSv7kUfTLiEhIYqPj9fOnTtd26qrq7Vz50637kZdqqqqdOTIEa/PHwEA4EuBPO0S6DyedklLS9OkSZM0ePBgDRkyxNWeufL0S3Jysjp16uR6RnnhwoUaNmyYevbsqfPnz2vZsmX6+uuv9fjjj3v3SgAA8KFAnnYJdB6HjwcffFBnz57V3Llz5XA4NHDgQH344Yeum1ALCwvd7qj99ttvlZKSIofDobZt2yo+Pl45OTnq06eP964CAADcMIKsGyB2OZ1OhYeHq7S0tF7PQaNxJScn+7sEXLZjxw5/l4DLrr4XDv7XWO8ZV96TDh06pNatWzfoWN99950GDRp0072/8cVyAAAYugH+/x6Q+GI5AADgU3Q+AAAwwA2n5ggfAAAYIHyYI3wAAGCA8GGOez4AAIBP0fkAAMAAnQ9zhA8AAAwQPswx7QIAAHyKzgcAAAbofJgjfAAAYIDwYY5pFwAA4FN0PgAAMEDnwxzhAwAAA4QPc0y7AAAAn6LzAQCAATof5ggfAAAYIHyYI3wAAGCA8GGOez4AAIBP0fkAAMAAnQ9zhA8AAAwQPswx7QIAAHyKzgcAAAbofJgjfAAAYIDwYY5pFwAA4FN0PgAAMEDnwxzhAwAAQzdreGgopl0AAGiCunXrpqCgILdlyZIlde7zww8/KDU1Ve3bt1erVq00fvx4lZSUeL02wgcAAAauTLs0dGlMCxcuVHFxsWv55S9/Wef4mTNn6g9/+IM2b96sPXv26C9/+Yt+9rOfeb0upl0AADBwI9zz0bp1a0VHR9drbGlpqdatW6eNGzfq7rvvliStX79evXv31r59+zRs2DCv1UXnAwAAA97sfDidTrelvLzcKzUuWbJE7du3V1xcnJYtW6ZLly7VOjYvL0+VlZUaNWqUa1tsbKy6dOmi3Nxcr9RzBZ0PAAD8LCYmxm193rx5mj9/foOO+fTTT2vQoEFq166dcnJylJ6eruLiYj3//PM1jnc4HAoJCVGbNm3ctkdFRcnhcDSolqsRPgAAMODNaZeioiLZbDbX9tDQ0BrHz549W0uXLq3zmMePH1dsbKzS0tJc2/r376+QkBA98cQTysjIqPX4vkL4AADAgDfDh81mcwsftZk1a5YmT55c55gePXrUuH3o0KG6dOmSTp06pV69el3z8+joaFVUVOj8+fNu3Y+SkpJ63zdSX4QPAABuEBEREYqIiDDaNz8/X8HBwYqMjKzx5/Hx8WrevLl27typ8ePHS5IKCgpUWFgou91uXHNNPL7hdO/evbr//vvVsWNHBQUFaevWrdfdZ/fu3Ro0aJBCQ0PVs2dPZWZmGpQKAEDgCORHbXNzc/XCCy/o888/15///Ge9+eabmjlzph555BG1bdtWknT69GnFxsZq//79kqTw8HBNmTJFaWlp+vjjj5WXl6fHHntMdrvdq0+6SAbho6ysTAMGDNDKlSvrNf6rr77S2LFjNWLECOXn52vGjBl6/PHHtX37do+LBQAgUARy+AgNDVVWVpaGDx+uvn37avHixZo5c6bWrl3rGlNZWamCggJdvHjRtW3FihX6p3/6J40fP1533XWXoqOj9e6773q9Po+nXe69917de++99R6/evVqde/eXc8995wkqXfv3vrkk0+0YsUKJSUleXp6AABwHYMGDdK+ffvqHNOtW7drwk9YWJhWrlxZ7waDqUb/nI/c3Fy3Z4YlKSkpqc5nhsvLy6955hkAgEASyJ2PQNfoN5w6HA5FRUW5bYuKipLT6dT333+vFi1aXLNPRkaGFixYcM32qVOnKiQkpNFqRf18/vnn/i4BCDhX/zsH/6murtbZs2cb/Tw3wiecBqqA/ITT9PR0lZaWupaioiJ/lwQAALyk0Tsf0dHR13wjXklJiWw2W41dD+nHG2X8/QEoAADUhc6HuUYPH3a7XR988IHbtuzsbK8/MwwAgC8RPsx5PO1y4cIF5efnKz8/X9KPj9Lm5+ersLBQ0o9TJsnJya7xTz75pP785z/r2Wef1Zdffqn//u//1ltvvaWZM2d65woAAPADbjg153H4OHjwoOLi4hQXFydJSktLU1xcnObOnStJKi4udgURSerevbvef/99ZWdna8CAAXruuef0yiuv8JgtAAA3KY+nXRITE+tMajV9emliYqIOHz7s6akAAAhYTLuY47tdAAAwQPgwF5CP2gIAgKaLzgcAAAbofJgjfAAAYIDwYY5pFwAA4FN0PgAAMEDnwxzhAwAAQzdreGgopl0AAIBP0fkAAMAA0y7mCB8AABggfJgjfAAAYIDwYY57PgAAgE/R+QAAwACdD3OEDwAADBA+zDHtAgAAfIrOBwAABuh8mCN8AABggPBhjmkXAADgU3Q+AAAwQOfDHOEDAAADhA9zTLsAAACfovMBAIABOh/mCB8AABggfJhj2gUAAANXwkdDl8awe/duBQUF1bgcOHCg1v0SExOvGf/kk096vT46HwAANDEJCQkqLi522/af//mf2rlzpwYPHlznvikpKVq4cKFrvWXLll6vj/ABAICBQJ52CQkJUXR0tGu9srJS7733nn75y18qKCiozn1btmzptm9jYNoFAAAD3px2cTqdbkt5eblXa/3973+vv/3tb3rssceuO/bNN9/UrbfeqjvuuEPp6em6ePGiV2uR6HwAAOB3MTExbuvz5s3T/PnzvXb8devWKSkpSZ07d65z3EMPPaSuXbuqY8eO+tOf/qRf/epXKigo0Lvvvuu1WiTCBwAARrw57VJUVCSbzebaHhoaWuP42bNna+nSpXUe8/jx44qNjXWtf/PNN9q+fbveeuut69YzdepU15/79eunDh06aOTIkTp58qRuu+226+5fX4QPAAAMeDN82Gw2t/BRm1mzZmny5Ml1junRo4fb+vr169W+fXv98z//s8f1DR06VJJ04sQJwgcAADejiIgIRURE1Hu8ZVlav369kpOT1bx5c4/Pl5+fL0nq0KGDx/vWhRtOAQAwEMif83HFrl279NVXX+nxxx+/5menT59WbGys9u/fL0k6efKkFi1apLy8PJ06dUq///3vlZycrLvuukv9+/f3al10PgAAMBDIj9pesW7dOiUkJLjdA3JFZWWlCgoKXE+zhISE6KOPPtILL7ygsrIyxcTEaPz48ZozZ47X6yJ8AADQRG3cuLHWn3Xr1s0t/MTExGjPnj2+KIvwAQCAqZv1u1kaivABAICBG2HaJVB5fMPp3r17df/996tjx44KCgrS1q1b6xxf25fbOBwO05oBAPC7G+GG00DlcfgoKyvTgAEDtHLlSo/2KygoUHFxsWuJjIz09NQAAKAJ8Hja5d5779W9997r8YkiIyPVpk2beo0tLy93+1x7p9Pp8fkAAGhMTLuY89nnfAwcOFAdOnTQPffco08//bTOsRkZGQoPD3ctV3/mPQAA/sa0i7lGDx8dOnTQ6tWr9c477+idd95RTEyMEhMTdejQoVr3SU9PV2lpqWspKipq7DIBAICPNPrTLr169VKvXr1c6wkJCTp58qRWrFihN954o8Z9QkNDa/1SHQAAAgHTLub88vHqQ4YM0YkTJ/xxagAAvIJpF3N+CR/5+fle/5IaAABwY/B42uXChQtuXYuvvvpK+fn5ateunbp06aL09HSdPn1ar7/+uiTphRdeUPfu3dW3b1/98MMPeuWVV7Rr1y7t2LHDe1cBAICPMe1izuPwcfDgQY0YMcK1npaWJkmaNGmSMjMzVVxcrMLCQtfPKyoqNGvWLJ0+fVotW7ZU//799dFHH7kdAwCAGw3hw5zH4SMxMbHOv6zMzEy39WeffVbPPvusx4UBABDICB/m/HLPBwAAuHnxxXIAABig82GO8AEAgAHChzmmXQAAgE/R+QAAwACdD3OEDwAADBA+zDHtAgAAfIrOBwAABuh8mCN8AABggPBhjmkXAADgU3Q+AAAwQOfDHOEDAAADhA9zhA8AAAzdrOGhobjnAwAA+BSdDwAADDDtYo7wAQCAAcKHOaZdAACAT9H5AADAAJ0Pc3Q+AAAwcCV8NHRpLIsXL1ZCQoJatmypNm3a1DimsLBQY8eOVcuWLRUZGalnnnlGly5dqvO4586d08MPPyybzaY2bdpoypQpunDhgke1ET4AAGiCKioqNGHCBD311FM1/ryqqkpjx45VRUWFcnJy9NprrykzM1Nz586t87gPP/ywjh07puzsbG3btk179+7V1KlTPaqNaRcAAAwE+rTLggULJEmZmZk1/nzHjh364osv9NFHHykqKkoDBw7UokWL9Ktf/Urz589XSEjINfscP35cH374oQ4cOKDBgwdLkl566SXdd999Wr58uTp27Fiv2uh8AABgwJvTLk6n020pLy9v9Ppzc3PVr18/RUVFubYlJSXJ6XTq2LFjte7Tpk0bV/CQpFGjRik4OFifffZZvc9N+AAAwM9iYmIUHh7uWjIyMhr9nA6Hwy14SHKtOxyOWveJjIx023bLLbeoXbt2te5TE6ZdAAAw4M1pl6KiItlsNtf20NDQGsfPnj1bS5curfOYx48fV2xsbIPqamyEDwAADHgzfNhsNrfwUZtZs2Zp8uTJdY7p0aNHvc4dHR2t/fv3u20rKSlx/ay2fc6cOeO27dKlSzp37lyt+9SE8AEAgAF/3HAaERGhiIiIBp3zCrvdrsWLF+vMmTOuqZTs7GzZbDb16dOn1n3Onz+vvLw8xcfHS5J27dql6upqDR06tN7n5p4PAACaoMLCQuXn56uwsFBVVVXKz89Xfn6+6zM5Ro8erT59+ujRRx/V559/ru3bt2vOnDlKTU11Tfvs379fsbGxOn36tCSpd+/eGjNmjFJSUrR//359+umnmjZtmn7+85/X+0kXic4HAABGAv1R27lz5+q1115zrcfFxUmSPv74YyUmJqpZs2batm2bnnrqKdntdv3kJz/RpEmTtHDhQtc+Fy9eVEFBgSorK13b3nzzTU2bNk0jR45UcHCwxo8fr9/97nce1Ub4AADAQKCHj8zMzFo/4+OKrl276oMPPqj154mJidfU2K5dO23cuLFBtTHtAgAAfIrOBwAABgK98xHICB8AABggfJhj2gUAAPgUnQ8AAAzQ+TBH+AAAwADhw5xH0y4ZGRm688471bp1a0VGRmrcuHEqKCi47n6bN29WbGyswsLC1K9fvzof6wEAAE2bR+Fjz549Sk1N1b59+5Sdna3KykqNHj1aZWVlte6Tk5OjiRMnasqUKTp8+LDGjRuncePG6ejRow0uHgAAf7nS+WjocjPyaNrlww8/dFvPzMxUZGSk8vLydNddd9W4z4svvqgxY8bomWeekSQtWrRI2dnZevnll7V69eoa9ykvL1d5eblr3el0elImAACNjmkXcw2656O0tFTSj592Vpvc3FylpaW5bUtKStLWrVtr3ScjI0MLFiy4ZvuuXbsUHMwDOsAVUVFR/i4Blw0YMMDfJeCyiooKbdq0ySfnulnDQ0MZv5NXV1drxowZ+ulPf6o77rij1nEOh+OafyCjoqLkcDhq3Sc9PV2lpaWupaioyLRMAAAQYIw7H6mpqTp69Kg++eQTb9YjSQoNDXV9ox4AAIGIaRdzRuFj2rRp2rZtm/bu3avOnTvXOTY6OlolJSVu20pKShQdHW1yagAAAgLhw5xH0y6WZWnatGnasmWLdu3ape7du193H7vdrp07d7pty87Olt1u96xSAADQJHjU+UhNTdXGjRv13nvvqXXr1q77NsLDw9WiRQtJUnJysjp16qSMjAxJ0vTp0zV8+HA999xzGjt2rLKysnTw4EGtXbvWy5cCAIDv0Pkw51HnY9WqVSotLVViYqI6dOjgWv7+ruLCwkIVFxe71hMSErRx40atXbtWAwYM0Ntvv62tW7fWeZMqAACBjs/5MOdR56M+f0m7d+++ZtuECRM0YcIET04FAACaKL7bBQAAA0y7mCN8AABggPBhjo8LBQAAPkXnAwAAA3Q+zBE+AAAwQPgwR/gAAMAA4cMc93wAAACfovMBAIABOh/mCB8AABggfJhj2gUAAPgUnQ8AAAzQ+TBH+AAAwADhwxzTLgAAwKfofAAAYIDOhzk6HwAAGLgSPhq6NJbFixcrISFBLVu2VJs2ba75+eeff66JEycqJiZGLVq0UO/evfXiiy9e97jdunVTUFCQ27JkyRKPaqPzAQBAE1RRUaEJEybIbrdr3bp11/w8Ly9PkZGR2rBhg2JiYpSTk6OpU6eqWbNmmjZtWp3HXrhwoVJSUlzrrVu39qg2wgcAAAYCfdplwYIFkqTMzMwaf/6LX/zCbb1Hjx7Kzc3Vu+++e93w0bp1a0VHRxvXxrQLAAAGvDnt4nQ63Zby8nK/XFNpaanatWt33XFLlixR+/btFRcXp2XLlunSpUsenYfOBwAAhrzVuYiJiXFbnzdvnubPn++VY9dXTk6ONm3apPfff7/OcU8//bQGDRqkdu3aKScnR+np6SouLtbzzz9f73MRPgAA8LOioiLZbDbXemhoaI3jZs+eraVLl9Z5rOPHjys2Ntaj8x89elQPPPCA5s2bp9GjR9c5Ni0tzfXn/v37KyQkRE888YQyMjJqrftqhA8AAAx4854Pm83mFj5qM2vWLE2ePLnOMT169PCohi+++EIjR47U1KlTNWfOHI/2laShQ4fq0qVLOnXqlHr16lWvfQgfAAAY8McNpxEREYqIiGjQOf/esWPHdPfdd2vSpElavHix0THy8/MVHBysyMjIeu9D+AAAoAkqLCzUuXPnVFhYqKqqKuXn50uSevbsqVatWuno0aO6++67lZSUpLS0NDkcDklSs2bNXAFn//79Sk5O1s6dO9WpUyfl5ubqs88+04gRI9S6dWvl5uZq5syZeuSRR9S2bdt610b4AADAQKA/ajt37ly99tprrvW4uDhJ0scff6zExES9/fbbOnv2rDZs2KANGza4xnXt2lWnTp2SJF28eFEFBQWqrKyU9OO9KFlZWZo/f77Ky8vVvXt3zZw50+0+kPoIsm6Az3Z1Op0KDw9XRESEgoN5Ohi4Iioqyt8l4LIBAwb4uwRcVlFRoU2bNqm0tLRe91F46sp70pQpUxQSEtKgY1VUVGjdunWNVmug4p0cAAD4FNMuAAAYCPRpl0BG+AAAwADhwxzTLgAAwKfofAAAYIDOhznCBwAABggf5ggfAAAYIHyY454PAADgU3Q+AAAwQOfDHOEDAAADhA9zHk27ZGRk6M4771Tr1q0VGRmpcePGqaCgoM59MjMzFRQU5LaEhYU1qGgAAHDj8ih87NmzR6mpqdq3b5+ys7NVWVmp0aNHq6ysrM79bDabiouLXcvXX3/doKIBAPC3K52Phi43I4+mXT788EO39czMTEVGRiovL0933XVXrfsFBQUpOjrarEIAAAIQ0y7mGvS0S2lpqSSpXbt2dY67cOGCunbtqpiYGD3wwAM6duxYnePLy8vldDrdFgAA0DQYh4/q6mrNmDFDP/3pT3XHHXfUOq5Xr1569dVX9d5772nDhg2qrq5WQkKCvvnmm1r3ycjIUHh4uGuJiYkxLRMAgEbBtIs54/CRmpqqo0ePKisrq85xdrtdycnJGjhwoIYPH653331XERERWrNmTa37pKenq7S01LUUFRWZlgkAQKMgfJgzetR22rRp2rZtm/bu3avOnTt7tG/z5s0VFxenEydO1DomNDRUoaGhJqUBAIAA51Hnw7IsTZs2TVu2bNGuXbvUvXt3j09YVVWlI0eOqEOHDh7vCwBAoKDzYc6jzkdqaqo2btyo9957T61bt5bD4ZAkhYeHq0WLFpKk5ORkderUSRkZGZKkhQsXatiwYerZs6fOnz+vZcuW6euvv9bjjz/u5UsBAMB3eNrFnEfhY9WqVZKkxMREt+3r16/X5MmTJUmFhYUKDv6/hsq3336rlJQUORwOtW3bVvHx8crJyVGfPn0aVjkAAH52s4aHhvIofNTnL3n37t1u6ytWrNCKFSs8KgoAADRdfLcLAAAGmHYxR/gAAMAA4cNcgz7hFAAAwFN0PgAAMEDnwxzhAwAAA4QPc0y7AAAAn6LzAQCAATof5ggfAAAYIHyYY9oFAAD4FJ0PAAAM0PkwR+cDAAADgf6ttosXL1ZCQoJatmypNm3a1DgmKCjomiUrK6vO4547d04PP/ywbDab2rRpoylTpujChQse1Ub4AADAQKCHj4qKCk2YMEFPPfVUnePWr1+v4uJi1zJu3Lg6xz/88MM6duyYsrOztW3bNu3du1dTp071qDamXQAAaIIWLFggScrMzKxzXJs2bRQdHV2vYx4/flwffvihDhw4oMGDB0uSXnrpJd13331avny5OnbsWK/j0PkAAMCANzsfTqfTbSkvL/fZdaSmpurWW2/VkCFD9Oqrr9bZjcnNzVWbNm1cwUOSRo0apeDgYH322Wf1PiedDwAADHjzhtOYmBi37fPmzdP8+fMbdOz6WLhwoe6++261bNlSO3bs0L/927/pwoULevrpp2sc73A4FBkZ6bbtlltuUbt27eRwOOp9XsIHAAB+VlRUJJvN5loPDQ2tcdzs2bO1dOnSOo91/PhxxcbG1uu8//mf/+n6c1xcnMrKyrRs2bJaw4e3ED4AADDgzc6HzWZzCx+1mTVrliZPnlznmB49ehjXM3ToUC1atEjl5eU1BqDo6GidOXPGbdulS5d07ty5et83IhE+AAAw4o/P+YiIiFBERESDzlmX/Px8tW3bttbOi91u1/nz55WXl6f4+HhJ0q5du1RdXa2hQ4fW+zyEDwAAmqDCwkKdO3dOhYWFqqqqUn5+viSpZ8+eatWqlf7whz+opKREw4YNU1hYmLKzs/Wb3/xG//7v/+46xv79+5WcnKydO3eqU6dO6t27t8aMGaOUlBStXr1alZWVmjZtmn7+85/X+0kXifABAICRQP+E07lz5+q1115zrcfFxUmSPv74YyUmJqp58+ZauXKlZs6cKcuy1LNnTz3//PNKSUlx7XPx4kUVFBSosrLSte3NN9/UtGnTNHLkSAUHB2v8+PH63e9+51FtQdYN8NmuTqdT4eHhioiIUHAwTwcDV0RFRfm7BFw2YMAAf5eAyyoqKrRp0yaVlpbW6z4KT115T0pKSlLz5s0bdKzKykpt37690WoNVLyTAwAAn2LaBQAAA4E+7RLICB8AABggfJgjfAAAYIDwYY57PgAAgE/R+QAAwNDN2rloKMIHAAAGmHYxx7QLAADwKTofAAAYoPNhjvABAIABwoc5pl0AAIBP0fkAAMAAnQ9zhA8AAAwQPswx7QIAAHzKo/CxatUq9e/fXzabTTabTXa7XX/84x/r3Gfz5s2KjY1VWFiY+vXrpw8++KBBBQMAEAiudD4autyMPAofnTt31pIlS5SXl6eDBw/q7rvv1gMPPKBjx47VOD4nJ0cTJ07UlClTdPjwYY0bN07jxo3T0aNHvVI8AAD+QvgwF2Q18MrbtWunZcuWacqUKdf87MEHH1RZWZm2bdvm2jZs2DANHDhQq1evrvc5nE6nwsPDFRERoeBgZoqAK6KiovxdAi4bMGCAv0vAZRUVFdq0aZNKS0tls9m8fvwr70n/+I//qFtuaditk5cuXdInn3zSaLUGKuN38qqqKmVlZamsrEx2u73GMbm5uRo1apTbtqSkJOXm5tZ57PLycjmdTrcFAAA0DR5HtiNHjshut+uHH35Qq1attGXLFvXp06fGsQ6H45r/mUVFRcnhcNR5joyMDC1YsOCa7WfPnvW0XDQC/rcdOPjfduB4/fXX/V0CLnM6ndq0aVOjn4enXcx53Pno1auX8vPz9dlnn+mpp57SpEmT9MUXX3i1qPT0dJWWlrqWoqIirx4fAICG4p4Pcx53PkJCQtSzZ09JUnx8vA4cOKAXX3xRa9asuWZsdHS0SkpK3LaVlJQoOjq6znOEhoYqNDTU09IAAMANoMF3b1ZXV6u8vLzGn9ntdu3cudNtW3Z2dq33iAAAcKOg82HOo85Henq67r33XnXp0kXfffedNm7cqN27d2v79u2SpOTkZHXq1EkZGRmSpOnTp2v48OF67rnnNHbsWGVlZengwYNau3at968EAAAf4p4Pcx6FjzNnzig5OVnFxcUKDw9X//79tX37dt1zzz2SpMLCQrdHYRMSErRx40bNmTNHv/71r3X77bdr69atuuOOO7x7FQAA4IbhUfhYt25dnT/fvXv3NdsmTJigCRMmeFQUAACBjs6HOb5YDgAAA4QPc3xcKAAA8Ck6HwAAGKDzYY7wAQCAAcKHOcIHAAAGCB/muOcDAAD4FOEDAABDgfzpposXL1ZCQoJatmypNm3aXPPzzMxMBQUF1bicOXOm1uN269btmvFLlizxqDamXQAAMBDo0y4VFRWaMGGC7HZ7jZ/T9eCDD2rMmDFu2yZPnqwffvhBkZGRdR574cKFSklJca23bt3ao9oIHwAANEELFiyQ9GOHoyYtWrRQixYtXOtnz57Vrl27rvuBotKPYeN6XxJbF6ZdAAAw4M0vlnM6nW5LbV/Y2phef/11tWzZUv/yL/9y3bFLlixR+/btFRcXp2XLlunSpUsenYvOBwAABrw57RITE+O2fd68eZo/f36Dju2pdevW6aGHHnLrhtTk6aef1qBBg9SuXTvl5OQoPT1dxcXFev755+t9LsIHAAB+VlRUJJvN5loPDQ2tcdzs2bO1dOnSOo91/PhxxcbGenT+3NxcHT9+XG+88cZ1x6alpbn+3L9/f4WEhOiJJ55QRkZGrXVfjfABAIABb3Y+bDabW/iozaxZszR58uQ6x/To0cPjOl555RUNHDhQ8fHxHu87dOhQXbp0SadOnVKvXr3qtQ/hAwAAA/542iUiIkIRERENOufVLly4oLfeeksZGRlG++fn5ys4OPi6T8j8PcIHAABNUGFhoc6dO6fCwkJVVVUpPz9fktSzZ0+1atXKNW7Tpk26dOmSHnnkkWuOsX//fiUnJ2vnzp3q1KmTcnNz9dlnn2nEiBFq3bq1cnNzNXPmTD3yyCNq27ZtvWsjfAAAYCDQP+dj7ty5eu2111zrcXFxkqSPP/5YiYmJru3r1q3Tz372sxo/iOzixYsqKChQZWWlpB/vRcnKytL8+fNVXl6u7t27a+bMmW73gdQH4QMAAAOBHj4yMzNr/YyPv5eTk1PrzxITE91qHDRokPbt29fg2ggfAAAYCPTwEcj4kDEAAOBTdD4AADBA58Mc4QMAAAOED3NMuwAAAJ+i8wEAgAE6H+YIHwAAGCB8mGPaBQAA+BSdDwAADND5MEf4AADAAOHDHNMuAADAp+h8AABggM6HOcIHAAAGCB/mCB8AABggfJjjng8AAOBTdD4AADB0s3YuGorwAQCAAaZdzDHtAgAAfMqj8LFq1Sr1799fNptNNptNdrtdf/zjH2sdn5mZqaCgILclLCyswUUDAOBvVzofDV1uRh5Nu3Tu3FlLlizR7bffLsuy9Nprr+mBBx7Q4cOH1bdv3xr3sdlsKigocK0HBQU1rGIAAAIA0y7mPAof999/v9v64sWLtWrVKu3bt6/W8BEUFKTo6GjzCgEAQJNifM9HVVWVsrKyVFZWJrvdXuu4CxcuqGvXroqJidEDDzygY8eOXffY5eXlcjqdbgsAAIGEaRdzHoePI0eOqFWrVgoNDdWTTz6pLVu2qE+fPjWO7dWrl1599VW999572rBhg6qrq5WQkKBvvvmmznNkZGQoPDzctcTExHhaJgAAjYrwYS7I8vDKKyoqVFhYqNLSUr399tt65ZVXtGfPnloDyN+rrKxU7969NXHiRC1atKjWceXl5SovL3etO51OAkgAiYqK8ncJuGz06NH+LgGXvf766/4uAZc5nU6Fh4ertLRUNput0Y4fExOj4OCGPTRaXV2toqKiRqs1UHn8OR8hISHq2bOnJCk+Pl4HDhzQiy++qDVr1lx33+bNmysuLk4nTpyoc1xoaKhCQ0M9LQ0AAJ/hhlNzDf6cj+rqarcuRV2qqqp05MgRdejQoaGnBQDAr5h2MedR5yM9PV333nuvunTpou+++04bN27U7t27tX37dklScnKyOnXqpIyMDEnSwoULNWzYMPXs2VPnz5/XsmXL9PXXX+vxxx/3/pUAAOBDdD7MeRQ+zpw5o+TkZBUXFys8PFz9+/fX9u3bdc8990iSCgsL3ea/vv32W6WkpMjhcKht27aKj49XTk5Ove4PAQAATZPHN5z6w5WbexAYuOE0cHDDaeDghtPA4asbTjt06OCVG06Li4u54RQAAFwf0y7m+GI5AADgU4QPAAAMBPLTLqdOndKUKVPUvXt3tWjRQrfddpvmzZuniooKt3F/+tOf9P/+3/9TWFiYYmJi9Nvf/va6xy4sLNTYsWPVsmVLRUZG6plnntGlS5c8qo9pFwAADATytMuXX36p6upqrVmzRj179tTRo0eVkpKisrIyLV++XNKP966MHj1ao0aN0urVq3XkyBH94he/UJs2bTR16tQaj1tVVaWxY8cqOjpaOTk5Ki4uVnJyspo3b67f/OY39a6P8AEAQBMzZswYjRkzxrXeo0cPFRQUaNWqVa7w8eabb6qiokKvvvqqQkJC1LdvX+Xn5+v555+vNXzs2LFDX3zxhT766CNFRUVp4MCBWrRokX71q19p/vz5CgkJqVd9TLsAAGDAm9MuV3+Zan0/vNMTpaWlateunWs9NzdXd911l1tgSEpKUkFBgb799tsaj5Gbm6t+/fq5PfWYlJQkp9NZry+OvYLwAQCAAW+Gj5iYGLcvVL3yYZ3ecuLECb300kt64oknXNscDsc1H51wZd3hcNR4HJN9akL4AADAz658udyVJT09vcZxs2fPVlBQUJ3Ll19+6bbP6dOnNWbMGE2YMEEpKSm+uJzr4p4PAAAMePOGU5vNVq8PGZs1a5YmT55c55gePXq4/vyXv/xFI0aMUEJCgtauXes2Ljo6WiUlJW7brqxHR0fXeOzo6Gjt37/fo31qQvgAAMCAP552iYiIUERERL3Gnj59WiNGjFB8fLzWr19/zaex2u12/cd//IcqKyvVvHlzSVJ2drZ69eqltm3b1nhMu92uxYsX68yZM4qMjHTtY7PZPPrqFKZdAAAwEMif83H69GklJiaqS5cuWr58uc6ePSuHw+F2X8ZDDz2kkJAQTZkyRceOHdOmTZv04osvKi0tzTVmy5Ytio2Nda2PHj1affr00aOPPqrPP/9c27dv15w5c5SamqrQ0NB610fnAwCAJiY7O1snTpzQiRMn1LlzZ7efXQk84eHh2rFjh1JTUxUfH69bb71Vc+fOdXvMtrS0VAUFBa71Zs2aadu2bXrqqadkt9v1k5/8RJMmTdLChQs9qo8vloPH+GK5wMEXywUOvlgucPjqi+Xatm2roKCgBh3Lsix9++23fLEcAAC4Pm/83/0G+P9/o+CeDwAA4FN0PgAAMEDnwxzhAwAAA4QPc0y7AAAAn6LzAQCAATof5ggfAAAYIHyYY9oFAAD4FJ0PAAAM0PkwR/gAAMAA4cMc4QMAAAOED3Pc8wEAAHyKzgcAAAbofJgjfAAAYIDwYY5pFwAA4FN0PgAAMEDnwxzhAwAAA4QPc0y7AAAAn6LzAQCAATof5ggfAAAYIHyYY9oFAAD4FJ0PAAAM0PkwR/gAAMAA4cNcg6ZdlixZoqCgIM2YMaPOcZs3b1ZsbKzCwsLUr18/ffDBBw05LQAAfmdZlleWm5Fx+Dhw4IDWrFmj/v371zkuJydHEydO1JQpU3T48GGNGzdO48aN09GjR01PDQAAbmBG0y4XLlzQww8/rP/5n//Rf/3Xf9U59sUXX9SYMWP0zDPPSJIWLVqk7Oxsvfzyy1q9enWN+5SXl6u8vNy1XlpaalImGkl1dbW/S8BlFRUV/i4BlzmdTn+XgMuuvBa+6CrcrJ2LBrMMJCcnWzNmzLAsy7KGDx9uTZ8+vdaxMTEx1ooVK9y2zZ071+rfv3+t+8ybN8+SxMLCwsLCYrycPHnS5C3uur7//nsrOjraa3VGR0db33//faPUGqg87nxkZWXp0KFDOnDgQL3GOxwORUVFuW2LioqSw+GodZ/09HSlpaW51s+fP6+uXbuqsLBQ4eHhnpYcEJxOp2JiYlRUVCSbzebvcoxxHYGjKVyD1DSuoylcg9R0rqO0tFRdunRRu3btGuX4YWFh+uqrr7zWeQwJCVFYWJhXjnWj8Ch8FBUVafr06crOzm7Uv6jQ0FCFhoZesz08PPyG/oWQJJvNdsNfg8R1BJKmcA1S07iOpnANUtO5juDgxvsoq7CwsJsuMHiTR+EjLy9PZ86c0aBBg1zbqqqqtHfvXr388ssqLy9Xs2bN3PaJjo5WSUmJ27aSkhJFR0c3oGwAAHCj8igWjhw5UkeOHFF+fr5rGTx4sB5++GHl5+dfEzwkyW63a+fOnW7bsrOzZbfbG1Y5AAC4IXnU+WjdurXuuOMOt20/+clP1L59e9f25ORkderUSRkZGZKk6dOna/jw4Xruuec0duxYZWVl6eDBg1q7dm29zxsaGqp58+bVOBVzo2gK1yBxHYGkKVyD1DSuoylcg8R1wHeCLKthzwklJiZq4MCBeuGFF1zr3bp1U2ZmpmvM5s2bNWfOHJ06dUq33367fvvb3+q+++5ryGkBAMANqsHhAwAAwBN8qy0AAPApwgcAAPApwgcAAPApwgcAAPCpgAkfK1euVLdu3RQWFqahQ4dq//79dY7fvHmzYmNjFRYWpn79+umDDz7wUaW18+QaMjMzFRQU5Lb4+9Py9u7dq/vvv18dO3ZUUFCQtm7det19du/erUGDBik0NFQ9e/Z0e8rJXzy9jt27d1/zWgQFBdX5FQCNLSMjQ3feeadat26tyMhIjRs3TgUFBdfdL9B+L0yuI9B+N1atWqX+/fu7PvXTbrfrj3/8Y537BNrrIHl+HYH2OtRkyZIlCgoK0owZM+ocF4ivx80uIMLHpk2blJaWpnnz5unQoUMaMGCAkpKSdObMmRrH5+TkaOLEiZoyZYoOHz6scePGady4cTp69KiPK/8/nl6D9ONHGBcXF7uWr7/+2ocVX6usrEwDBgzQypUr6zX+q6++0tixYzVixAjl5+drxowZevzxx7V9+/ZGrrRunl7HFQUFBW6vR2RkZCNVeH179uxRamqq9u3bp+zsbFVWVmr06NEqKyurdZ9A/L0wuQ4psH43OnfurCVLligvL08HDx7U3XffrQceeEDHjh2rcXwgvg6S59chBdbrcLUDBw5ozZo16t+/f53jAvX1uOn593vtfjRkyBArNTXVtV5VVWV17NjRysjIqHH8v/7rv1pjx4512zZ06FDriSeeaNQ66+LpNaxfv94KDw/3UXWek2Rt2bKlzjHPPvus1bdvX7dtDz74oJWUlNSIlXmmPtfx8ccfW5Ksb7/91ic1mThz5owlydqzZ0+tYwLx9+Jq9bmOQP/dsCzLatu2rfXKK6/U+LMb4XW4oq7rCOTX4bvvvrNuv/12Kzs7+7rfrH4jvR43E793PioqKpSXl6dRo0a5tgUHB2vUqFHKzc2tcZ/c3Fy38ZKUlJRU6/jGZnINknThwgV17dpVMTEx1/0fSCAKtNehoQYOHKgOHTronnvu0aeffurvctyUlpZKUp3f0nkjvB71uQ4pcH83qqqqlJWVpbKyslq/IuJGeB3qcx1S4L4OqampGjt27DV/zzW5EV6Pm5Hfw8df//pXVVVVKSoqym17VFRUrXPuDofDo/GNzeQaevXqpVdffVXvvfeeNmzYoOrqaiUkJOibb77xRcleUdvr4HQ69f333/upKs916NBBq1ev1jvvvKN33nlHMTExSkxM1KFDh/xdmiSpurpaM2bM0E9/+tNrvt7g7wXa78XV6nsdgfi7ceTIEbVq1UqhoaF68skntWXLFvXp06fGsYH8OnhyHYH4OkhSVlaWDh065PoKj+sJ5NfjZubRd7vAe+x2u9v/OBISEtS7d2+tWbNGixYt8mNlN59evXqpV69ervWEhASdPHlSK1as0BtvvOHHyn6Umpqqo0eP6pNPPvF3KQ1S3+sIxN+NXr16KT8/X6WlpXr77bc1adIk7dmzp9Y37kDlyXUE4utQVFSk6dOnKzs7O+BufoVn/B4+br31VjVr1kwlJSVu20tKShQdHV3jPtHR0R6Nb2wm13C15s2bKy4uTidOnGiMEhtFba+DzWZTixYt/FSVdwwZMiQg3uynTZumbdu2ae/evercuXOdYwPt9+LveXIdVwuE342QkBD17NlTkhQfH68DBw7oxRdf1Jo1a64ZG8ivgyfXcbVAeB3y8vJ05swZDRo0yLWtqqpKe/fu1csvv6zy8vJrvl09kF+Pm5nfp11CQkIUHx+vnTt3urZVV1dr586dtc5F2u12t/GSlJ2dXefcZWMyuYarVVVV6ciRI+rQoUNjlel1gfY6eFN+fr5fXwvLsjRt2jRt2bJFu3btUvfu3a+7TyC+HibXcbVA/N2orq5WeXl5jT8LxNehNnVdx9UC4XUYOXKkjhw5ovz8fNcyePBgPfzww8rPz78meEg31utxU/H3Ha+WZVlZWVlWaGiolZmZaX3xxRfW1KlTrTZt2lgOh8OyLMt69NFHrdmzZ7vGf/rpp9Ytt9xiLV++3Dp+/Lg1b948q3nz5taRI0f8dQkeX8OCBQus7du3WydPnrTy8vKsn//851ZYWJh17Ngxf12C9d1331mHDx+2Dh8+bEmynn/+eevw4cPW119/bVmWZc2ePdt69NFHXeP//Oc/Wy1btrSeeeYZ6/jx49bKlSutZs2aWR9++KG/LsGyLM+vY8WKFdbWrVut//3f/7WOHDliTZ8+3QoODrY++ugjf12C9dRTT1nh4eHW7t27reLiYtdy8eJF15gb4ffC5DoC7Xdj9uzZ1p49e6yvvvrK+tOf/mTNnj3bCgoKsnbs2FFj/YH4OliW59cRaK9Dba5+2uVGeT1udgERPizLsl566SWrS5cuVkhIiDVkyBBr3759rp8NHz7cmjRpktv4t956y/qHf/gHKyQkxOrbt6/1/vvv+7jia3lyDTNmzHCNjYqKsu677z7r0KFDfqj6/1x55PTq5UrdkyZNsoYPH37NPgMHDrRCQkKsHj16WOvXr/d53Vfz9DqWLl1q3XbbbVZYWJjVrl07KzEx0dq1a5d/ir+spvoluf393gi/FybXEWi/G7/4xS+srl27WiEhIVZERIQ1cuRI1xu2Zd0Yr4NleX4dgfY61Obq8HGjvB43uyDLsizf9VkAAMDNzu/3fAAAgJsL4QMAAPgU4QMAAPgU4QMAAPgU4QMAAPgU4QMAAPgU4QMAAPgU4QMAAPgU4QMAAPgU4QMAAPgU4QMAAPjU/wc7n5AHS4z7nwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_gridworld_value(V):\n",
    "    plt.figure()\n",
    "    c = plt.pcolormesh(V, cmap='gray')\n",
    "    plt.colorbar(c)\n",
    "    plt.gca().invert_yaxis()  # In the array, first row = 0 is on top\n",
    "\n",
    "# Making a plot always helps\n",
    "plot_gridworld_value(V.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Policy Iteration (2 points)\n",
    "Using the policy evaluation algorithm we can implement policy iteration to find a good policy for this problem. Note that we do not need to use a discount_factor for episodic tasks but make sure your implementation can handle this correctly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dp_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dp_autograde.py\n",
    "\n",
    "def policy_iter_v(env, policy_eval_v=policy_eval_v, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Policy Iteration Algorithm. Iteratively evaluates and improves a policy\n",
    "    until an optimal policy is found.\n",
    "    \n",
    "    Args:\n",
    "        env: The OpenAI envrionment.\n",
    "        policy_eval_v: Policy Evaluation function that takes 3 arguments:\n",
    "            policy, env, discount_factor.\n",
    "        discount_factor: gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, V). \n",
    "        policy is the optimal policy, a matrix of shape [S, A] where each state s\n",
    "        contains a valid probability distribution over actions.\n",
    "        V is the value function for the optimal policy.\n",
    "        \n",
    "    \"\"\"\n",
    "    # Start with a random policy\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Let's see what it does\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m policy, v \u001b[39m=\u001b[39m policy_iter_v(env, policy_eval_v)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPolicy Probability Distribution:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(policy)\n",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m, in \u001b[0;36mpolicy_iter_v\u001b[0;34m(env, policy_eval_v, discount_factor)\u001b[0m\n\u001b[1;32m     20\u001b[0m policy \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mones([env\u001b[39m.\u001b[39mnS, env\u001b[39m.\u001b[39mnA]) \u001b[39m/\u001b[39m env\u001b[39m.\u001b[39mnA\n\u001b[1;32m     21\u001b[0m \u001b[39m# YOUR CODE HERE\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[39mreturn\u001b[39;00m policy, V\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Let's see what it does\n",
    "policy, v = policy_iter_v(env, policy_eval_v)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "def print_grid_policy(policy, symbols=[\"^\", \">\", \"v\", \"<\"]):\n",
    "    symbols = np.array(symbols)\n",
    "    for row in policy:\n",
    "        print(\"\".join(symbols[row]))\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(env.shape))\n",
    "print(\"\")\n",
    "\n",
    "plot_gridworld_value(v.reshape(env.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Q-value Iteration (3 points)\n",
    "In this exercise you will implement the value iteration algorithm. However, because this algorithm is quite similar to the ones you implemented previously, we will spice things up a bit and use Q-values instead. Thus instead of using Bellman optimality equations for V you will use Bellman equations for Q. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Appending to dp_autograde.py\n"
     ]
    }
   ],
   "source": [
    "%%execwritefile -a dp_autograde.py\n",
    "\n",
    "def value_iter_q(env, theta=0.0001, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    Q-value Iteration Algorithm.\n",
    "    \n",
    "    Args:\n",
    "        env: OpenAI env. env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment. \n",
    "            env.nA is a number of actions in the environment.\n",
    "        theta: We stop evaluation once our value function change is less than theta for all state-action pairs.\n",
    "        discount_factor: Gamma discount factor.\n",
    "        \n",
    "    Returns:\n",
    "        A tuple (policy, Q) of the optimal policy and the optimal Q-value function.        \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # Start with an all 0 Q-value function\n",
    "    Q = np.zeros((env.nS, env.nA))\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    # Initialize delta (change in Q) to be larger than theta\n",
    "    delta = -1\n",
    "    \n",
    "    # Start with a random policy (uniform distribution over actions)\n",
    "    policy = np.ones([env.nS, env.nA]) / env.nA\n",
    "    \n",
    "    # Repeat until convergence\n",
    "    while (delta > theta or delta < 0):\n",
    "        \n",
    "        # Loop over all states\n",
    "        Q_old = np.copy(Q)\n",
    "        for s in range(env.nS):\n",
    "            \n",
    "            # Calculate the expected value of state s under the current policy\n",
    "            for a, phi_prob in enumerate(policy[s]):\n",
    "                for prob, next_state, reward, done in env.P[s][a]:\n",
    "                    \n",
    "                    # Gives the value of the action for the maximum Q-value in the next state, so the most optimal action\n",
    "                    for next_s in next_state:\n",
    "                        next_Q = np.array([max(Q[next_s])])\n",
    "                    \n",
    "                        # Update the Q-value function for state s and action a\n",
    "                        Q[s] = phi_prob * (rewards + discount_factor * next_Q)\n",
    "            \n",
    "            # Get the largest difference in between the old and new Q-value function\n",
    "            delta = np.max(abs(Q - Q_old))\n",
    "                     \n",
    "\n",
    "    # For this exercise, we should not generate a stochastic policy, but a one-hot deterministic policy.\n",
    "    # Stochastic policy could be done by taking the softmax of the Q array along dim 1.\n",
    "    \n",
    "    policy = np.zeros([env.nS, env.nA])\n",
    "    for s in range(env.nS):\n",
    "        action_choice = np.argmax(Q[s,:])\n",
    "        policy[s, action_choice] = 1\n",
    "    \n",
    "    return policy, Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Let's see what it does\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m policy, Q \u001b[39m=\u001b[39m value_iter_q(env)\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mPolicy Probability Distribution:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(policy)\n",
      "Cell \u001b[0;32mIn[36], line 41\u001b[0m, in \u001b[0;36mvalue_iter_q\u001b[0;34m(env, theta, discount_factor)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m a, phi_prob \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(policy[s]):\n\u001b[1;32m     38\u001b[0m     \u001b[39mfor\u001b[39;00m prob, next_state, reward, done \u001b[39min\u001b[39;00m env\u001b[39m.\u001b[39mP[s][a]:\n\u001b[1;32m     39\u001b[0m         \n\u001b[1;32m     40\u001b[0m         \u001b[39m# Gives the value of the action for the maximum Q-value in the next state, so the most optimal action\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m         \u001b[39mfor\u001b[39;00m next_s \u001b[39min\u001b[39;00m next_state:\n\u001b[1;32m     42\u001b[0m             next_Q \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([\u001b[39mmax\u001b[39m(Q[next_s])])\n\u001b[1;32m     44\u001b[0m         \u001b[39m# Update the Q-value function for state s and action a\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "# Let's see what it does\n",
    "policy, Q = value_iter_q(env)\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print_grid_policy(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Q Function:\")\n",
    "print(Q)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'venv' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "# As you can see, the visualization of the Q function is quite clumsy and is not that easy to check \n",
    "# that all values make sense. However, you can easily create a V function from Q and policy to double\n",
    "# check that the values are what you would expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to test/submit your solution **restart the kernel, run all cells and submit the dp_autograde.py file into codegrade.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
